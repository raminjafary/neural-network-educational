{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for file handling and HTTP requests\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# Define the data directory path\n",
    "DATA_PATH = Path('data')\n",
    "# Create full path for mnist data by joining 'data' and 'mnist'\n",
    "PATH = DATA_PATH / 'mnist'\n",
    "\n",
    "# Create the directory structure, making parent directories if needed\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the base URL where the MNIST dataset is hosted\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "# Define the filename of the MNIST dataset\n",
    "FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "# Check if the file doesn't already exist\n",
    "if not (PATH / FILENAME).exists():\n",
    "    # Download the content from the URL\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    # Write the downloaded content to a file in binary mode\n",
    "    (PATH / FILENAME).open('wb').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# Import pickle module for loading serialized data\n",
    "import pickle\n",
    "# Import gzip module for reading compressed files\n",
    "import gzip\n",
    "\n",
    "# Open the gzipped MNIST file in binary read mode\n",
    "# Convert Path object to string using as_posix() for compatibility\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    # Load the pickle data with latin-1 encoding\n",
    "    # Unpack into training data (x_train, y_train), validation data (x_valid, y_valid)\n",
    "    # and ignore the test data (using _)\n",
    "    ((x_train, y_train), (x_valid, y_valid),\n",
    "     _) = pickle.load(f, encoding='latin-1')\n",
    "\n",
    "# Print shape of training data features (number of images x pixels per image)\n",
    "print(x_train.shape)\n",
    "# Print shape of training data labels (number of labels)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# Import numpy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Display the first training image by:\n",
    "# - Taking the first image (x_train[0])\n",
    "# - Reshaping it from 1D (784) to 2D (28x28) array\n",
    "# - Using grayscale colormap\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray')\n",
    "# Print the corresponding label (digit) for the first training image\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch library for deep learning operations\n",
    "import torch\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "# map() applies torch.tensor to each array (x_train, y_train, x_valid, y_valid)\n",
    "# This converts our training and validation data from NumPy arrays to PyTorch tensors\n",
    "# which allows us to use them with PyTorch's deep learning operations\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import math module for mathematical operations\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility (like np.random.seed(0) in numpy)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create random weights matrix of shape (784, 10) and normalize by sqrt(784)\n",
    "# In numpy this would be: weights = np.random.randn(784, 10) / np.sqrt(784)\n",
    "# The dimensions (784, 10) are used because when we multiply:\n",
    "# - input data (batch_size=50000, features=784) by\n",
    "# - weights (features=784, classes=10)\n",
    "# we get (50000, 10) which gives us predictions for each class.\n",
    "# Note: If using (10, 784) weights, we'd need to transpose the input\n",
    "# to (784, 50000) for x * w + b instead of w * x + b\n",
    "# - 784 is the input size (28x28 pixels flattened)\n",
    "# - 10 is the output size (digits 0-9)\n",
    "#\n",
    "# PyTorch and numpy use different dimension ordering conventions:\n",
    "# - PyTorch uses (input_size, output_size) for weights\n",
    "# - Numpy commonly uses (output_size, input_size)\n",
    "#\n",
    "# But they represent the same mathematical operation for matrix multiplication\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "\n",
    "# Enable gradient tracking for weights - numpy arrays don't have this capability\n",
    "weights.requires_grad_()\n",
    "\n",
    "# Create zero vector for biases with gradient tracking enabled\n",
    "# In numpy this would be: biases = np.zeros(10)\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our simple linear model function\n",
    "# xb: input batch of images (each flattened to 784 pixels)\n",
    "# Returns: raw logits (pre-softmax scores) for each class\n",
    "def model(xb):\n",
    "    # Matrix multiply input (xb) with weights and add bias\n",
    "    # xb shape: (batch_size, 784)\n",
    "    # weights shape: (784, 10)\n",
    "    # bias shape: (10,)\n",
    "    # Output shape: (batch_size, 10)\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Softmax function converts raw logits to probabilities\n",
    "# Formula: softmax(x_i) = exp(x_i) / sum(exp(x_j)) for j=1...n\n",
    "# Where:\n",
    "# - x_i is the input value for class i\n",
    "# - exp() is the exponential function\n",
    "# - sum() is over all classes\n",
    "#\n",
    "# For numerical stability, we should subtract max(x) from all values first\n",
    "# (not implemented here for simplicity)\n",
    "#\n",
    "# The function:\n",
    "# 1. Applies exp() to get positive values\n",
    "# 2. Normalizes by dividing by sum of all exp values\n",
    "# 3. Uses unsqueeze(-1) to maintain proper broadcasting\n",
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum(-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "# Cross Entropy Loss Function\n",
    "# Formula: -1/N * Σ(y_i * log(p_i)) where:\n",
    "# - N is batch size (number of samples)\n",
    "# - y_i is target probability (1 for correct class, 0 for others)\n",
    "# - p_i is predicted probability for class i\n",
    "# - Σ sums over all classes for each sample, then over all samples\n",
    "#\n",
    "# One-hot encoding converts integer class labels to binary vectors:\n",
    "# e.g. for 10 classes:\n",
    "# class 3 -> [0,0,0,1,0,0,0,0,0,0]\n",
    "# class 7 -> [0,0,P0,0,0,0,0,1,0,0]\n",
    "# This allows element-wise operations with predictions\n",
    "def cross_entropy_loss(pred, targets):\n",
    "    # Get batch size (bs) and number of classes (out_features) from prediction shape\n",
    "    # pred shape: (batch_size, num_classes)\n",
    "    bs, out_features = pred.shape\n",
    "\n",
    "    # Convert integer targets to one-hot vectors using identity matrix indexing\n",
    "    # torch.eye(out_features) creates identity matrix of size (out_features, out_features)\n",
    "    # Indexing with [targets] selects rows corresponding to target classes\n",
    "    # Result shape: (batch_size, num_classes)\n",
    "    one_hot_encoded_targets = torch.eye(out_features)[targets]\n",
    "\n",
    "    # Calculate cross entropy loss:\n",
    "    # 1. softmax(pred) converts logits to probabilities\n",
    "    # 2. .log() takes natural log of probabilities\n",
    "    # 3. multiply by one-hot targets (zeros out non-target classes)\n",
    "    # 4. .sum() adds up all values (both across classes and batch)\n",
    "    # 5. divide by batch size to get average loss per sample\n",
    "    return -(one_hot_encoded_targets * softmax(pred).log()).sum() / bs\n",
    "\n",
    "\n",
    "# Accuracy Function - Compares model predictions to actual labels\n",
    "# Input:\n",
    "#   pred: Model predictions (logits) with shape (batch_size, num_classes)\n",
    "#   yb: True labels as integers with shape (batch_size)\n",
    "# Returns: Boolean tensor indicating correct predictions\n",
    "def accuracy_func(pred, yb):\n",
    "    # torch.argmax finds the index of the maximum value along dimension 1 (the class dimension)\n",
    "    # Example for one sample:\n",
    "    # pred = [0.1, 0.7, 0.2] (logits/probabilities for 3 classes)\n",
    "    # argmax returns 1 since 0.7 is the highest value\n",
    "    #\n",
    "    # For a batch, if pred is:\n",
    "    # [[0.1, 0.7, 0.2],\n",
    "    #  [0.3, 0.2, 0.5]]\n",
    "    # argmax returns [1, 2] - class 1 for first sample, class 2 for second sample\n",
    "    pred_class = torch.argmax(pred, dim=1)\n",
    "\n",
    "    # Compare predicted classes to true labels\n",
    "    # Example:\n",
    "    # pred_class = [1, 2]\n",
    "    # yb =         [1, 1]\n",
    "    # Returns:     [True, False]\n",
    "    # True for correct predictions, False for incorrect ones\n",
    "    # Compare predicted classes (pred_class) with true labels (yb) element-wise\n",
    "    # (pred_class == yb) creates a boolean tensor: True for correct predictions, False for incorrect\n",
    "    # .float() converts booleans to 0.0 (False) and 1.0 (True)\n",
    "    # .mean() calculates the average, giving us the accuracy as a fraction between 0 and 1\n",
    "    return (pred_class == yb).float().mean()\n",
    "\n",
    "\n",
    "print(softmax(model(x_train))[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
      "loss=2.4608447551727295\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "bs = 64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "pred = model(xb)\n",
    "yb = y_train[0:bs]\n",
    "\n",
    "print(F.cross_entropy(pred, yb))\n",
    "print(f\"loss={cross_entropy_loss(pred, yb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.460845 Accuracy: 4.7%\n",
      "Loss: 0.117402 Accuracy: 98.4%\n",
      "Loss: 0.121539 Accuracy: 96.9%\n",
      "Loss: 0.059987 Accuracy: 100.0%\n",
      "Loss: 0.080367 Accuracy: 98.4%\n",
      "Loss: 0.098814 Accuracy: 98.4%\n",
      "Loss: 0.049722 Accuracy: 100.0%\n",
      "Loss: 0.079958 Accuracy: 96.9%\n",
      "Loss: 0.462700 Accuracy: 90.6%\n",
      "Loss: 0.040384 Accuracy: 100.0%\n",
      "Loss: 0.070740 Accuracy: 98.4%\n",
      "Loss: 0.020320 Accuracy: 100.0%\n",
      "Loss: 0.030883 Accuracy: 100.0%\n",
      "Loss: 0.033094 Accuracy: 100.0%\n",
      "Loss: 0.030932 Accuracy: 100.0%\n",
      "Loss: 0.026909 Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "epochs = 2          # Number of complete passes through the training dataset\n",
    "bs = 64            # Batch size - number of samples processed in each iteration\n",
    "n = x_train.shape[0]  # Total number of training samples\n",
    "# Number of batches per epoch (adding 1 to handle remaining samples)\n",
    "num_batches = n // bs + 1\n",
    "lr = 0.5          # Learning rate - controls how much we adjust the weights in each step\n",
    "\n",
    "# Training Loop\n",
    "# We iterate through the dataset multiple times (epochs) to improve model performance\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_batches):\n",
    "        # Calculate batch indices\n",
    "        start_i = i\n",
    "        end_i = start_i + bs\n",
    "\n",
    "        # Get current batch of data\n",
    "        xb = x_train[start_i:end_i]    # Input images (batch_size x 784 pixels)\n",
    "        yb = y_train[start_i:end_i]    # True labels (batch_size)\n",
    "\n",
    "        # Forward Pass\n",
    "        # Model predictions: shape (batch_size x 10)\n",
    "        pred = model(xb)\n",
    "        # Example: [[0.1, 0.2, ..., 0.05], ...] for each digit 0-9\n",
    "\n",
    "        # Calculate Loss and Accuracy\n",
    "        # Cross Entropy Loss Formula: -Σ(y_true * log(y_pred))\n",
    "        # For example, if true label is 7:\n",
    "        # y_true = [0,0,0,0,0,0,0,1,0,0]\n",
    "        # y_pred = [0.1,0.05,0.1,0.05,0.1,0.1,0.1,0.3,0.05,0.05]\n",
    "        # loss = -log(0.3) ≈ 1.20\n",
    "        loss = cross_entropy_loss(pred, yb)\n",
    "\n",
    "        # Accuracy calculation:\n",
    "        # 1. Find predicted class (argmax of predictions)\n",
    "        # 2. Compare with true labels\n",
    "        # Example: if pred_class=[7,2,1] and true_labels=[7,2,0]\n",
    "        # accuracy = 2/3 ≈ 0.67 (2 correct predictions out of 3)\n",
    "        accuracy = accuracy_func(pred, yb)\n",
    "\n",
    "        # Backward Pass - Compute gradients\n",
    "        loss.backward()  # Uses autograd to compute ∂loss/∂weights and ∂loss/∂bias\n",
    "\n",
    "        # Gradient Descent Update\n",
    "        # Formula: w = w - lr * ∂loss/∂w\n",
    "        # Example: if weight=0.5, gradient=0.1, lr=0.5\n",
    "        # New weight = 0.5 - (0.5 * 0.1) = 0.45\n",
    "        with torch.no_grad():  # Temporarily disable gradient tracking\n",
    "            weights -= weights.grad * lr  # Update weights\n",
    "            bias -= bias.grad * lr        # Update bias\n",
    "\n",
    "            # Reset gradients to zero\n",
    "            weights.grad.zero_()  # Prevent gradient accumulation\n",
    "            bias.grad.zero_()\n",
    "\n",
    "        # Print training progress every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            train_loss, train_accuracy = loss.item(), accuracy.item() * 100\n",
    "            print(f\"Loss: {train_loss:6f} Accuracy: {train_accuracy:0.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
